{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_lab_activity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "easvwzo-K5AZ"
      },
      "source": [
        "from __future__ import print_function, division\r\n",
        "from tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, Dropout, MaxPooling2D, Activation, ZeroPadding2D\r\n",
        "from tensorflow.python.keras.layers.convolutional import UpSampling2D, Conv2D\r\n",
        "from tensorflow.python.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVLqGS__K8Nn"
      },
      "source": [
        "def discriminator():\r\n",
        "   start = Input(shape = (image_size,image_size,1))\r\n",
        "\r\n",
        "   d1 = Conv2D(8, kernel_size=3, strides=2, input_shape=(image_size,image_size,1), padding=\"same\")(start)\r\n",
        "   d1 = LeakyReLU(alpha=0.2)(d1)\r\n",
        "\r\n",
        "   d2 = Conv2D(16, kernel_size=3, strides=2, padding=\"same\")(d1)\r\n",
        "   d2 = ZeroPadding2D(padding=((0, 1), (0, 1)))(d2)\r\n",
        "   d2 = LeakyReLU(alpha=0.2)(d2)\r\n",
        "\r\n",
        "   d3 = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d2)\r\n",
        "   d3 = LeakyReLU(alpha=0.2)(d3)\r\n",
        "\r\n",
        "   d4 = Flatten()(d3)\r\n",
        "   output = Dense(1, activation='sigmoid')(d4)\r\n",
        "\r\n",
        "   return Model(start, output)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DUPvDG0K-R3"
      },
      "source": [
        "def generator(input_dim=32):\r\n",
        "   noise = Input(shape=(input_dim,))\r\n",
        "\r\n",
        "   g1 = Dense(7*7*32, activation=\"relu\", input_dim=input_dim)(noise)\r\n",
        "   g1 = Reshape((7, 7, 32))(g1)\r\n",
        "\r\n",
        "   g2 = UpSampling2D()(g1)\r\n",
        "   g2 = Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(g2)\r\n",
        "\r\n",
        "   g3 = UpSampling2D()(g2)\r\n",
        "   g3 = Conv2D(16, kernel_size=3, padding=\"same\", activation=\"relu\")(g3)\r\n",
        "\r\n",
        "   g4 = Conv2D(1, kernel_size=3, padding=\"same\")(g3)\r\n",
        "   img = Activation(\"tanh\")(g4)\r\n",
        "\r\n",
        "   return Model(noise, img)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaLBtb25DGhL"
      },
      "source": [
        "def classifier():\r\n",
        "    start = Input(shape = (image_size,image_size,1))\r\n",
        "    \r\n",
        "    c1 = Conv2D(8, kernel_size=3, strides=2, input_shape=(image_size,image_size,1), padding='same', activation='relu')(start)\r\n",
        "    c1 = MaxPooling2D(pool_size=(2, 2))(c1)\r\n",
        "\r\n",
        "    c2 = Conv2D(16, kernel_size=3, strides=2, input_shape=(image_size,image_size,1), padding='same', activation='relu')(c1)\r\n",
        "    c2 = MaxPooling2D(pool_size=(2, 2))(c2)\r\n",
        "\r\n",
        "    flat = Flatten()(c2)\r\n",
        "    hidden1 = Dense(64, activation='relu')(flat)\r\n",
        "    output = Dense(2, activation='softmax')(hidden1)\r\n",
        "\r\n",
        "    return Model(inputs=start, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgVt1f485Twx"
      },
      "source": [
        "def obtain_dataset(selected_class, number_of_images):\r\n",
        "  # load data from mnist\r\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "  x_train = x_train[y_train==selected_class][:int(number_of_images*(1-test_size)),:,:]\r\n",
        "  y_train = y_train[y_train==selected_class][:int(number_of_images*(1-test_size))]\r\n",
        "  x_test = x_test[y_test==selected_class][:int(number_of_images*test_size),:,:]\r\n",
        "  y_test = y_test[y_test==selected_class][:int(number_of_images*test_size)]\r\n",
        "  \r\n",
        "  # Set data shape, type and groundtruth\r\n",
        "  x_train = x_train.astype('float32').reshape(-1, image_size, image_size, 1)\r\n",
        "  x_train /= 255\r\n",
        "\r\n",
        "  x_test = x_test.astype('float32').reshape(-1, image_size, image_size, 1)\r\n",
        "  x_test /= 255\r\n",
        "\r\n",
        "  y_train = y_train.astype('uint8').reshape((-1,1))\r\n",
        "  y_test = y_test.astype('uint8').reshape((-1,1))\r\n",
        "\r\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZVOqQVdIrYC"
      },
      "source": [
        "def build_and_train_classifier(epochs, batch_size):\r\n",
        "  # build a classifier named 'model'\r\n",
        "  model = classifier()\r\n",
        "\r\n",
        "  # compile 'model'\r\n",
        "  optimizer_model = Adam(0.0001, 0.5)\r\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer_model, metrics=['accuracy'])\r\n",
        "\r\n",
        "  # train the classifier\r\n",
        "  history = model.fit(x_train, y_train, epochs=epochs, validation_split=0.1)\r\n",
        "\r\n",
        "  # test the classifier\r\n",
        "  y_pred = model.predict(x_test)\r\n",
        "  y_pred = y_pred.argmax(axis=-1)\r\n",
        "  \r\n",
        "  print('\\nconfusion matrix:\\n', confusion_matrix(y_test, y_pred))\r\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, y_pred)\r\n",
        "\r\n",
        "  plt.plot(fpr,tpr)\r\n",
        "  plt.xlabel('False Positive Rate')\r\n",
        "  plt.ylabel('True Positive Rate')\r\n",
        "  plt.title('ROC Curve')\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "  auc_ = auc(fpr, tpr)\r\n",
        "  return auc_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWQgKXFALEce"
      },
      "source": [
        "def build_and_train_GAN(epochs, batch_size):\r\n",
        "  \r\n",
        "  # define hyper-parameter for GAN\r\n",
        "  optimizer_GEN = Adam(0.0001, 0.5)\r\n",
        "  optimizer_DIS = Adam(0.0004, 0.5)\r\n",
        "\r\n",
        "  # build a discriminator named 'Dis' \r\n",
        "  Dis = discriminator()\r\n",
        "\r\n",
        "  # compile 'Dis'\r\n",
        "  Dis.compile(loss='binary_crossentropy', optimizer=optimizer_DIS, metrics=['accuracy'])\r\n",
        "\r\n",
        "  # build a generator named 'Gen' with input random_vector\r\n",
        "  Gen = generator(random_vector)\r\n",
        "\r\n",
        "  # Generator training route\r\n",
        "  start = Input(shape=(random_vector,))\r\n",
        "  fake_image = Gen(start)\r\n",
        "  Dis.trainable = False\r\n",
        "  decide = Dis(fake_image)\r\n",
        "  comb_model = Model(start, decide)\r\n",
        "  comb_model.compile(loss='binary_crossentropy', optimizer=optimizer_GEN)\r\n",
        "\r\n",
        "  valid = np.ones((batch_size, 1))\r\n",
        "  fake = np.zeros((batch_size, 1))\r\n",
        "\r\n",
        "  # Use train_on_batch instead of model.fit\r\n",
        "  for epoch in range(epochs):\r\n",
        "      count = 0\r\n",
        "      for time in range(x_train_0.shape[0]//batch_size):\r\n",
        "          # Get Real&Fake sample for discriminator from dataset&generator\r\n",
        "          noise = np.random.normal(0, 1, (batch_size, random_vector))\r\n",
        "          gen_images = Gen.predict(noise)\r\n",
        "          train_images = x_train_0[count:count+batch_size, :, :, :]\r\n",
        "          # Training Discriminator\r\n",
        "          d_loss_real = Dis.train_on_batch(train_images, valid)\r\n",
        "          d_loss_fake = Dis.train_on_batch(gen_images, fake)\r\n",
        "          d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "          # Training generator\r\n",
        "          noise = np.random.normal(0, 1, (batch_size, random_vector))\r\n",
        "          g_loss = comb_model.train_on_batch(noise, valid)\r\n",
        "\r\n",
        "          print(\"%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, time, d_loss[0], 100*d_loss[1], g_loss))\r\n",
        "\r\n",
        "          count += batch_size\r\n",
        "\r\n",
        "  return Gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJwDpp6cK5gy"
      },
      "source": [
        "def create_fake_images(number_of_fake_images):\r\n",
        "  fake_images = np.empty((0,image_size,image_size,1))\r\n",
        "  for ni in range(number_of_fake_images):\r\n",
        "    noise = np.random.normal(0, 1, (1,random_vector))\r\n",
        "    temp = Gen.predict(noise)\r\n",
        "    temp = 0.5 * temp + 0.5\r\n",
        "    fake_images = np.append(fake_images,temp)\r\n",
        "  \r\n",
        "  return fake_images\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9sXAzWzQfUS"
      },
      "source": [
        "# [1] Define the hyper-parameters\r\n",
        "test_size = 0.1\r\n",
        "image_size = 28 \r\n",
        "random_vector = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sX9-s8YQiRy"
      },
      "source": [
        "# [2] Obtain dataset '0' and '1' from mnist\r\n",
        "x_train_0, y_train_0, x_test_0, y_test_0 = obtain_dataset(selected_class=0, number_of_images=100)\r\n",
        "x_train_1, y_train_1, x_test_1, y_test_1 = obtain_dataset(selected_class=1, number_of_images=1000)\r\n",
        "print(\"class 0: \", x_train_0.shape, x_test_0.shape, y_train_0.shape, y_test_0.shape)\r\n",
        "print(\"class 1: \", x_train_1.shape, x_test_1.shape, y_train_1.shape, y_test_1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf1beTw7QkkS"
      },
      "source": [
        "# [3] Create imbalanced dataset by merging class '0' and '1'\r\n",
        "x_train=np.concatenate((x_train_0, x_train_1))\r\n",
        "x_test=np.concatenate((x_test_0, x_test_1))\r\n",
        "y_train=np.concatenate((y_train_0, y_train_1))\r\n",
        "y_test=np.concatenate((y_test_0, y_test_1))\r\n",
        "print(\"merged 0 and 1 data: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoOdE2LUQohp"
      },
      "source": [
        "# [4] Build, train, and test a classifier \r\n",
        "# TODO: call function build_and_train_classifier with epoch 5 and batch size 32, print the output \r\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBRCMaWvQuri"
      },
      "source": [
        "# [5] Build and train GAN\r\n",
        "# TODO: call function build_and_train_GAN with epoch 20 and batch size 32, save the output to a variable named 'Gen'\r\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyl7ZLTPQ3dR"
      },
      "source": [
        "# [6] Create fake data for class '0'\r\n",
        "number_of_fake_images = 900\r\n",
        "# TODO: call function create_fake_images with given number of fake images, save the output to a variable named 'fake_images'\r\n",
        "# ...\r\n",
        "fake_images = fake_images.reshape((-1, image_size, image_size, 1))\r\n",
        "print(fake_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbS8IdhyRESk"
      },
      "source": [
        "# [7] Merge fake data with the original data\r\n",
        "train_num = int(number_of_fake_images*(1-test_size))\r\n",
        "test_num = int(number_of_fake_images*test_size)\r\n",
        "x_train = np.concatenate((x_train, fake_images[:train_num,:,:]))\r\n",
        "x_test = np.concatenate((x_test, fake_images[train_num:,:,:]))\r\n",
        "y_train = np.concatenate((y_train, np.zeros(train_num).reshape((-1,1))))\r\n",
        "y_test = np.concatenate((y_test, np.zeros(test_num).reshape((-1,1))))\r\n",
        "print(\"final data: \", x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBzSdDk_Q_Vi"
      },
      "source": [
        "# [8] Build, train, and test a classifier\r\n",
        "# TODO: call function build_and_train_classifier with epoch 5 and batch size 32, print the output \r\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0iFVY526I5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}